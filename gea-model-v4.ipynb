{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# GAE",
   "id": "74f7ed965b90fc08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "a511aa2d122e9de1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load datasets\n",
    "job_descriptions = pd.read_csv('./data/processed/job_descriptions_processed.csv')\n",
    "resumes = pd.read_csv('./data/processed/general-resume-dataset-processed-v1.csv', converters={'skills': literal_eval})\n",
    "\n",
    "# Shuffle job_descriptions and select the first N rows\n",
    "job_descriptions = job_descriptions.sample(frac=1, random_state=random_seed).head(50000)\n",
    "\n",
    "# Convert 'skills' column to list\n",
    "job_descriptions['skills'] = job_descriptions['skills'].apply(literal_eval)"
   ],
   "id": "fa9cfddac8b5ada2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace None values in job_title and category with a default value before encoding\n",
    "job_descriptions['job_title'].fillna('unknown', inplace=True)\n",
    "resumes['job_title'].fillna('unknown', inplace=True)\n",
    "resumes['category'].fillna('unknown', inplace=True)\n",
    "\n",
    "# Ensure job_ids and candidate_ids are correctly assigned\n",
    "job_descriptions['job_id'] = range(1, len(job_descriptions) + 1)\n",
    "resumes['candidate_id'] = range(1, len(resumes) + 1)\n",
    "\n",
    "# Add 'unknown' to the list of all titles and categories to handle unseen labels\n",
    "all_titles = job_descriptions['job_title'].tolist() + resumes['job_title'].tolist()\n",
    "all_titles.append('unknown')\n",
    "all_categories = resumes['category'].tolist()\n",
    "all_categories.append('unknown')"
   ],
   "id": "b70a742ea5564a74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit the label encoders\n",
    "le_job_title = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "le_job_title.fit(all_titles)\n",
    "le_category.fit(all_categories)\n",
    "\n",
    "# Transform the columns\n",
    "job_descriptions['job_title'] = le_job_title.transform(job_descriptions['job_title'])\n",
    "resumes['job_title'] = le_job_title.transform(resumes['job_title'])\n",
    "resumes['category'] = le_category.transform(resumes['category'])\n",
    "\n",
    "# Encode skills\n",
    "all_skills = set(skill for skills in job_descriptions['skills'].tolist() + resumes['skills'].tolist() for skill in skills)\n",
    "le_skills = {skill: i for i, skill in enumerate(all_skills)}"
   ],
   "id": "c49775725ec99b91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create nodes and edges for the graph\n",
    "nodes = []\n",
    "edges = []\n",
    "weights = []\n",
    "node_features = []\n",
    "\n",
    "jobs_from_edges = []\n",
    "candidates_from_edges = []\n",
    "jobs_and_candidates_from_edges = []\n",
    "\n",
    "skill_weight_multiplier = 1  # Weight for skill overlap\n",
    "title_weight = 5  # Weight for job title match\n",
    "\n",
    "# Add job nodes\n",
    "for i, row in job_descriptions.iterrows():\n",
    "    nodes.append(row['job_id'])\n",
    "    skills_vector = [0] * len(le_skills)\n",
    "    if row['skills']:  # Check if skills are not empty\n",
    "        for skill in row['skills']:\n",
    "            skills_vector[le_skills[skill]] = 1\n",
    "    node_features.append([row['job_title']] + skills_vector)\n",
    "\n",
    "# Add resume nodes, using 'category' instead of 'job_title'\n",
    "for i, row in resumes.iterrows():\n",
    "    nodes.append(row['candidate_id'] + len(job_descriptions))\n",
    "    skills_vector = [0] * len(le_skills)\n",
    "    if row['skills']:  # Check if skills are not empty\n",
    "        for skill in row['skills']:\n",
    "            skills_vector[le_skills[skill]] = 1\n",
    "    node_features.append([row['job_title']] + skills_vector)"
   ],
   "id": "5e3e9b0edc1243a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def skills_overlap(job_skills, resume_skills):\n",
    "    return len(set(job_skills).intersection(set(resume_skills)))\n",
    "\n",
    "threshold = 1  # Set a threshold for the minimum number of overlapping skills\n",
    "\n",
    "for i, job in job_descriptions.iterrows():\n",
    "    for j, resume in resumes.iterrows():\n",
    "        overlap = skills_overlap(job['skills'], resume['skills'])\n",
    "        combined_weight = overlap * skill_weight_multiplier\n",
    "\n",
    "        if job['job_title'] == resume['job_title']:\n",
    "            combined_weight += title_weight  # Add weight for job title match\n",
    "\n",
    "        if combined_weight > 0:\n",
    "            edges.append((job['job_id'], resume['candidate_id'] + len(job_descriptions)))\n",
    "            weights.append(combined_weight)\n",
    "            jobs_and_candidates_from_edges.append((job['job_id'], resume['candidate_id']))\n",
    "            jobs_from_edges.append(job['job_id'])\n",
    "            candidates_from_edges.append(resume['candidate_id'])\n",
    "\n",
    "nodes_length = len(nodes)"
   ],
   "id": "450b763c61d912f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pd.DataFrame(node_features).to_csv('./data/features/raw/nodes.csv')\n",
    "# pd.DataFrame(node_features).to_csv('./data/features/raw/node_features.csv')\n",
    "# pd.DataFrame(edges).to_csv('./data/features/raw/edges.csv')\n",
    "# pd.DataFrame(weights).to_csv('./data/features/raw/weights.csv')"
   ],
   "id": "c0873107071189ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "edge_index = edge_index.clamp(0, nodes_length - 1)"
   ],
   "id": "f4b872c380c1063b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(edge_index, './data/features/edge_index-50k.pt')\n",
    "torch.save(edge_weight, './data/features/edge_weight-50k.pt')\n",
    "with open('./data/features/nodes_len.txt', 'w') as file:\n",
    "    file.write(str(len(nodes)))"
   ],
   "id": "6221488afb2fe330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# edge_index = torch.load('./data/features/edge_index-50k.pt')\n",
    "# edge_weight = torch.load('./data/features/edge_weight-50k.pt')\n",
    "# nodes_length = 0\n",
    "# with open('./data/features/nodes_len.txt', 'r') as file:\n",
    "#     nodes_length = np.int64(file.read())"
   ],
   "id": "fedaf4988a9294bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# jobs_from_edges = set(jobs_from_edges)\n",
    "# candidates_from_edges = set(candidates_from_edges)\n",
    "# jobs_and_candidates_from_edges = set(jobs_and_candidates_from_edges)"
   ],
   "id": "70223d36bff3da48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pd.DataFrame(jobs_from_edges).to_csv('./data/features/raw/jobs_from_edges.csv')\n",
    "# pd.DataFrame(candidates_from_edges).to_csv('./data/features/raw/candidates_from_edges.csv')\n",
    "# pd.DataFrame(jobs_and_candidates_from_edges).to_csv('./data/features/raw/jobs_and_candidates_from_edges.csv')"
   ],
   "id": "49b551c0b4a198db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert node features to tensor\n",
    "x = torch.tensor(node_features, dtype=torch.float)"
   ],
   "id": "3550945e1238da95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(x, './data/features/x-50k.pt')",
   "id": "a21ff20f9cd3368f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# x = torch.load('./data/features/x-50k.pt')",
   "id": "c620e7b3c4d55285",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "\n",
    "# Ensure edge indices are within range\n",
    "# data.num_nodes = nodes_length\n",
    "\n",
    "original_edge_index = data.edge_index.clone()\n",
    "original_edge_weight = data.edge_weight.clone()\n",
    "\n",
    "# Splitting edges for training/validation\n",
    "data = train_test_split_edges(data)\n",
    "\n",
    "# Create a dictionary to map edge indices to their weights\n",
    "edge_weight_dict = {tuple(edge_index[:, i].tolist()): edge_weight[i].item() for i in range(edge_index.size(1))}\n",
    "\n",
    "def get_edge_weights(edge_index, edge_weight_dict):\n",
    "    weights = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        edge = tuple(edge_index[:, i].tolist())\n",
    "        weight = edge_weight_dict.get(edge, 0)  # Default to 0 if edge not found\n",
    "        weights.append(weight)\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ],
   "id": "92a9f77324216cda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_edge_weights = get_edge_weights(data.train_pos_edge_index, edge_weight_dict)\n",
    "test_edge_weights = get_edge_weights(data.test_pos_edge_index, edge_weight_dict)\n",
    "val_edge_weights = get_edge_weights(data.val_pos_edge_index, edge_weight_dict)"
   ],
   "id": "3482204b0f2c8be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data.train_pos_edge_weight = train_edge_weights\n",
    "data.test_pos_edge_weight = test_edge_weights\n",
    "data.val_pos_edge_weight = val_edge_weights\n",
    "\n",
    "# Manually create negative edges for training\n",
    "neg_edge_index_train = negative_sampling(\n",
    "    edge_index=data.train_pos_edge_index,\n",
    "    num_nodes=data.num_nodes,\n",
    "    num_neg_samples=data.train_pos_edge_index.size(1),\n",
    ")\n",
    "data.train_neg_edge_index = neg_edge_index_train\n",
    "\n",
    "# Assign zero weights to negative edges for training\n",
    "neg_train_edge_weights = torch.zeros(neg_edge_index_train.size(1), dtype=torch.float)\n",
    "\n",
    "# Manually create negative edges for testing\n",
    "neg_edge_index_test = negative_sampling(\n",
    "    edge_index=data.test_pos_edge_index,\n",
    "    num_nodes=data.num_nodes,\n",
    "    num_neg_samples=data.test_pos_edge_index.size(1),\n",
    ")\n",
    "data.test_neg_edge_index = neg_edge_index_test\n",
    "\n",
    "# Assign zero weights to negative edges for testing\n",
    "neg_test_edge_weights = torch.zeros(neg_edge_index_test.size(1), dtype=torch.float)\n",
    "\n",
    "# Combine positive and negative edge weights for training\n",
    "data.train_neg_edge_weight = neg_train_edge_weights\n",
    "\n",
    "# Combine positive and negative edge weights for testing\n",
    "data.test_neg_edge_weight = neg_test_edge_weights\n",
    "\n",
    "# Ensure edge_index tensors are of integer type\n",
    "data.train_pos_edge_index = data.train_pos_edge_index.long()\n",
    "data.test_pos_edge_index = data.test_pos_edge_index.long()\n",
    "data.train_neg_edge_index = data.train_neg_edge_index.long()\n",
    "data.test_neg_edge_index = data.test_neg_edge_index.long()"
   ],
   "id": "91f230b61ba4cc8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(original_edge_index.shape)\n",
    "print(original_edge_weight.shape)\n",
    "print(data.train_pos_edge_index.shape)\n",
    "print(train_edge_weights.shape)"
   ],
   "id": "73389fede9e486fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define number of nodes\n",
    "num_nodes = data.num_nodes\n",
    "\n",
    "# Node2Vec parameters\n",
    "embedding_dim = 64\n",
    "walk_length = 20\n",
    "context_size = 10\n",
    "walks_per_node = 10\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "num_epochs = 21"
   ],
   "id": "fba21fbb8d038909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Node2Vec\n",
    "node2vec = Node2Vec(\n",
    "    edge_index=original_edge_index,\n",
    "    embedding_dim=embedding_dim,\n",
    "    walk_length=walk_length,\n",
    "    context_size=context_size,\n",
    "    walks_per_node=walks_per_node,\n",
    "    num_negative_samples=1,\n",
    "    p=1,\n",
    "    q=1,\n",
    "    sparse=True,\n",
    "    num_nodes=num_nodes  # Specify number of nodes\n",
    ")\n",
    "\n",
    "# Move to the appropriate device (CPU/GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "node2vec = node2vec.to(device)"
   ],
   "id": "6f325c1adced2d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimizer for Node2Vec\n",
    "optimizer = torch.optim.SparseAdam(node2vec.parameters(), lr=lr)"
   ],
   "id": "d2b1d3e64f0c57b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop for Node2Vec\n",
    "def train_node2vec(num_epochs):\n",
    "    node2vec.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        loader = node2vec.loader(batch_size=batch_size, shuffle=True, num_workers=0)  # Set num_workers=0\n",
    "        for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = node2vec.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i}, Loss: {total_loss / 10}')\n",
    "                total_loss = 0"
   ],
   "id": "a4673e0d68813981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train Node2Vec model\n",
    "train_node2vec(num_epochs)"
   ],
   "id": "36416a190e418fdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract embeddings\n",
    "node_embeddings = node2vec.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "# Update node features with embeddings\n",
    "data.x = torch.tensor(node_embeddings, dtype=torch.float)"
   ],
   "id": "4f2994a9c90aef5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GAE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GAE, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_weight):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        return self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        pos_pred = (z[pos_edge_index[0].long()] * z[pos_edge_index[1].long()]).sum(dim=1)\n",
    "        neg_pred = (z[neg_edge_index[0].long()] * z[neg_edge_index[1].long()]).sum(dim=1)\n",
    "        return pos_pred, neg_pred\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.train_pos_edge_index, data.train_pos_edge_weight)\n",
    "        return z"
   ],
   "id": "280a1a320e603e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize and train GAE model as before\n",
    "model = GAE(data.num_node_features, 16)  # Adjust dimensions as needed\n",
    "gae_optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ],
   "id": "d9923f5f9b0fdb86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(data):\n",
    "    model.train()\n",
    "    gae_optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.train_pos_edge_index, data.train_pos_edge_weight)  # Pass train_pos_edge_weight\n",
    "    pos_pred, neg_pred = model.decode(z, data.train_pos_edge_index, data.train_neg_edge_index)\n",
    "    pos_loss = loss_fn(pos_pred, torch.ones_like(pos_pred))\n",
    "    neg_loss = loss_fn(neg_pred, torch.zeros_like(neg_pred))\n",
    "    loss = pos_loss + neg_loss\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(\"Warning: NaN or Inf loss detected\")\n",
    "        return float('inf')\n",
    "    loss.backward()\n",
    "    gae_optimizer.step()\n",
    "    return loss.item()"
   ],
   "id": "d59e18cf369aff0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(1000):\n",
    "    loss = train(data)\n",
    "    if loss == float('inf'):\n",
    "        break\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')"
   ],
   "id": "819793948be2b1e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(data, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.val_pos_edge_index, data.val_pos_edge_weight)\n",
    "        pos_pred = torch.sigmoid((z[data.val_pos_edge_index[0].long()] * z[data.val_pos_edge_index[1].long()]).sum(dim=1)).cpu().numpy()\n",
    "        neg_pred = torch.sigmoid((z[data.val_neg_edge_index[0].long()] * z[data.val_neg_edge_index[1].long()]).sum(dim=1)).cpu().numpy()\n",
    "\n",
    "    y_true = np.concatenate([np.ones(pos_pred.shape[0]), np.zeros(neg_pred.shape[0])])\n",
    "    y_pred = np.concatenate([pos_pred, neg_pred])\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    return auc_roc, ap"
   ],
   "id": "dd65d2d7892ba243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "auc_roc, ap = evaluate_model(data, model)\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}, AP: {ap:.4f}\")"
   ],
   "id": "37b2a03df1f5ce83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_best_candidates(job_descriptions, resumes, z):\n",
    "    job_ids = job_descriptions['job_id'].values\n",
    "    candidate_ids = resumes['candidate_id'].values + len(job_descriptions)\n",
    "\n",
    "    predictions = []\n",
    "    for job_id, job_title, job_skills in zip(job_ids, job_descriptions['job_title'], job_descriptions['skills']):\n",
    "        best_match = None\n",
    "        best_score = float('-inf')\n",
    "        for candidate_id, category, candidate_job_title, skills in zip(\n",
    "                resumes['candidate_id'], resumes['category'], resumes['job_title'], resumes['skills']):\n",
    "            candidate_index = candidate_id + len(job_descriptions) - 1\n",
    "            job_index = job_id - 1\n",
    "\n",
    "            # Ensure indices are within range\n",
    "            if candidate_index >= z.size(0) or job_index >= z.size(0):\n",
    "                continue\n",
    "\n",
    "            score = torch.sigmoid(torch.dot(z[job_index], z[candidate_index])).item()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = (job_id, job_title, candidate_id, candidate_job_title, category, skills, score)\n",
    "\n",
    "        if best_match:\n",
    "            job_id, job_title, candidate_id, candidate_job_title, category, skills, score = best_match\n",
    "            match_percentage = score * 100  # Assuming the score is between 0 and 1\n",
    "            predictions.append({\n",
    "                \"Job ID\": job_id,\n",
    "                \"Job Title\": le_job_title.inverse_transform([job_title])[0],\n",
    "                \"Candidate ID\": candidate_id,\n",
    "                \"Candidate Job Title\": le_job_title.inverse_transform([candidate_job_title])[0],\n",
    "                \"Candidate Category\": le_category.inverse_transform([category])[0],\n",
    "                \"Match Percentage\": match_percentage,\n",
    "                \"Mutual Skills\": set(job_skills).intersection(set(skills)),\n",
    "                \"Job Skills\": job_skills,\n",
    "                \"Candidate Skills\": skills\n",
    "            })\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    return predictions_df"
   ],
   "id": "d9b4e3dd15402801",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "with torch.no_grad():\n",
    "    z = model.encode(data.x, original_edge_index, data.edge_weight)"
   ],
   "id": "455e4c57053c84e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "jobs_to_predict = job_descriptions[job_descriptions['job_title'].isin(resumes['job_title'])].sample(frac=1, random_state=random_seed)[:100]",
   "id": "fed5a321cb5ba810",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions_df = predict_best_candidates(jobs_to_predict, resumes, z)",
   "id": "7a2ba7424bc80603",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions_df.head(100)",
   "id": "1930b82d20689a44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), f\"./models/gea-recommendation-system-50k-{auc_roc:.2f}-acc-{uuid.uuid4()}-{time.strftime('%Y%m%d-%H%M%S')}-v4.pth\")",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3d6f035b4107326f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
