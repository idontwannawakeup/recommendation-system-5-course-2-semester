{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# GAE",
   "id": "74f7ed965b90fc08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "a511aa2d122e9de1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load datasets\n",
    "job_descriptions = pd.read_csv('./data/processed/job_descriptions_processed-v4.csv')\n",
    "resumes = pd.read_csv('./data/processed/general-resume-dataset-processed-v4.csv', converters={'skills': literal_eval})\n",
    "\n",
    "# Shuffle job_descriptions and select the first N rows\n",
    "job_descriptions = job_descriptions.sample(frac=1, random_state=random_seed).head(10000)\n",
    "\n",
    "# Convert 'skills' column to list\n",
    "job_descriptions['skills'] = job_descriptions['skills'].apply(literal_eval)"
   ],
   "id": "fa9cfddac8b5ada2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace None values in job_title and category with a default value before encoding\n",
    "job_descriptions['job_title'].fillna('unknown', inplace=True)\n",
    "resumes['job_title'].fillna('unknown', inplace=True)\n",
    "resumes['category'].fillna('unknown', inplace=True)\n",
    "\n",
    "# Ensure job_ids and candidate_ids are correctly assigned\n",
    "job_descriptions['job_id'] = range(1, len(job_descriptions) + 1)\n",
    "resumes['candidate_id'] = range(1, len(resumes) + 1)\n",
    "\n",
    "# Add 'unknown' to the list of all titles and categories to handle unseen labels\n",
    "all_titles = job_descriptions['job_title'].tolist() + resumes['job_title'].tolist()\n",
    "all_titles.append('unknown')\n",
    "all_categories = resumes['category'].tolist()\n",
    "all_categories.append('unknown')"
   ],
   "id": "b70a742ea5564a74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit the label encoders\n",
    "le_job_title = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "le_job_title.fit(all_titles)\n",
    "le_category.fit(all_categories)\n",
    "\n",
    "# Transform the columns\n",
    "job_descriptions['job_title'] = le_job_title.transform(job_descriptions['job_title'])\n",
    "resumes['job_title'] = le_job_title.transform(resumes['job_title'])\n",
    "resumes['category'] = le_category.transform(resumes['category'])\n",
    "\n",
    "# Encode skills\n",
    "all_skills = set(skill for skills in job_descriptions['skills'].tolist() + resumes['skills'].tolist() for skill in skills)\n",
    "le_skills = {skill: i for i, skill in enumerate(all_skills)}"
   ],
   "id": "c49775725ec99b91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create nodes and edges for the graph\n",
    "nodes = []\n",
    "edges = []\n",
    "node_features = []\n",
    "\n",
    "jobs_from_edges = []\n",
    "candidates_from_edges = []\n",
    "jobs_and_candidates_from_edges = []\n",
    "\n",
    "skill_weight_multiplier = 3  # Weight for skill overlap\n",
    "title_weight = 15  # Weight for job title match\n",
    "\n",
    "# Add job nodes\n",
    "for i, row in job_descriptions.iterrows():\n",
    "    nodes.append(row['job_id'])\n",
    "    skills_vector = [0] * len(le_skills)\n",
    "    if row['skills']:  # Check if skills are not empty\n",
    "        for skill in row['skills']:\n",
    "            skills_vector[le_skills[skill]] = 1\n",
    "    node_features.append([row['job_title']] + skills_vector)\n",
    "\n",
    "# Add resume nodes, using 'category' instead of 'job_title'\n",
    "for i, row in resumes.iterrows():\n",
    "    nodes.append(row['candidate_id'] + len(job_descriptions))\n",
    "    skills_vector = [0] * len(le_skills)\n",
    "    if row['skills']:  # Check if skills are not empty\n",
    "        for skill in row['skills']:\n",
    "            skills_vector[le_skills[skill]] = 1\n",
    "    node_features.append([row['job_title']] + skills_vector)"
   ],
   "id": "5e3e9b0edc1243a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert job_skills and resume_skills to sets once\n",
    "job_descriptions['skills'] = job_descriptions['skills'].apply(set)\n",
    "resumes['skills'] = resumes['skills'].apply(set)\n",
    "\n",
    "# Create dictionaries for quick lookup\n",
    "job_skills_dict = job_descriptions.set_index('job_id')['skills'].to_dict()\n",
    "resume_skills_dict = resumes.set_index('candidate_id')['skills'].to_dict()\n",
    "job_titles_dict = job_descriptions.set_index('job_id')['job_title'].to_dict()\n",
    "resume_titles_dict = resumes.set_index('candidate_id')['job_title'].to_dict()\n",
    "\n",
    "edges = []\n",
    "weights = []\n",
    "\n",
    "for job_id, job_skills in job_skills_dict.items():\n",
    "    for candidate_id, resume_skills in resume_skills_dict.items():\n",
    "        overlap = len(job_skills.intersection(resume_skills))\n",
    "        combined_weight = overlap * skill_weight_multiplier\n",
    "\n",
    "        if job_titles_dict[job_id] == resume_titles_dict[candidate_id]:\n",
    "            combined_weight += title_weight  # Add weight for job title match\n",
    "\n",
    "        if combined_weight > 0:\n",
    "            edges.append((job_id, candidate_id + len(job_descriptions)))\n",
    "            weights.append(combined_weight)\n",
    "            jobs_and_candidates_from_edges.append((job_id, candidate_id))\n",
    "            jobs_from_edges.append(job_id)\n",
    "            candidates_from_edges.append(candidate_id)\n",
    "\n",
    "nodes_length = len(nodes)"
   ],
   "id": "f46e1bb964b76a01",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m     combined_weight \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m title_weight  \u001B[38;5;66;03m# Add weight for job title match\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m combined_weight \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 23\u001B[0m     edges\u001B[38;5;241m.\u001B[39mappend((job_id, candidate_id \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(job_descriptions)))\n\u001B[0;32m     24\u001B[0m     weights\u001B[38;5;241m.\u001B[39mappend(combined_weight)\n\u001B[0;32m     25\u001B[0m     jobs_and_candidates_from_edges\u001B[38;5;241m.\u001B[39mappend((job_id, candidate_id))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\recommendation-system-v6\\Lib\\site-packages\\pandas\\core\\frame.py:1647\u001B[0m, in \u001B[0;36mDataFrame.__len__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1643\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__len__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m   1644\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1645\u001B[0m \u001B[38;5;124;03m    Returns length of info axis, but here we use the index.\u001B[39;00m\n\u001B[0;32m   1646\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1647\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\recommendation-system-v6\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:909\u001B[0m, in \u001B[0;36mIndex.__len__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    900\u001B[0m         c\n\u001B[0;32m    901\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munique(level\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)[: get_option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay.max_dir_items\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[0;32m    902\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m c\u001B[38;5;241m.\u001B[39misidentifier()\n\u001B[0;32m    903\u001B[0m     }\n\u001B[0;32m    905\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;66;03m# Array-Like Methods\u001B[39;00m\n\u001B[0;32m    907\u001B[0m \n\u001B[0;32m    908\u001B[0m \u001B[38;5;66;03m# ndarray compat\u001B[39;00m\n\u001B[1;32m--> 909\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__len__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    910\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    911\u001B[0m \u001B[38;5;124;03m    Return the length of the Index.\u001B[39;00m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    913\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "edge_index = edge_index.clamp(0, nodes_length - 1)"
   ],
   "id": "f4b872c380c1063b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(edge_index, './data/features/edge_index-10k-a03.pt')\n",
    "torch.save(edge_weight, './data/features/edge_weight-10k-a03.pt')\n",
    "with open('./data/features/nodes_len-10k-a03.txt', 'w') as file:\n",
    "    file.write(str(len(nodes)))"
   ],
   "id": "6221488afb2fe330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# edge_index = torch.load('./data/features/edge_index-10k-a03.pt')\n",
    "# edge_weight = torch.load('./data/features/edge_weight-10k-a03.pt')\n",
    "# nodes_length = 0\n",
    "# with open('./data/features/nodes_len-10k-a03.txt', 'r') as file:\n",
    "#     nodes_length = np.int64(file.read())"
   ],
   "id": "fedaf4988a9294bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert node features to tensor\n",
    "x = torch.tensor(node_features, dtype=torch.float)"
   ],
   "id": "3550945e1238da95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(x, './data/features/x-10k-a03.pt')",
   "id": "a21ff20f9cd3368f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# x = torch.load('./data/features/x-10k-a03.pt')",
   "id": "c620e7b3c4d55285",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "\n",
    "original_edge_index = data.edge_index.clone()\n",
    "\n",
    "# Splitting edges for training/validation\n",
    "data = train_test_split_edges(data)"
   ],
   "id": "88519b6f8f6d76b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a dictionary to map edge indices to their weights\n",
    "edge_weight_dict = {tuple(edge_index[:, i].tolist()): edge_weight[i].item() for i in range(edge_index.size(1))}\n",
    "\n",
    "def get_edge_weights(edge_index, edge_weight_dict):\n",
    "    weights = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        edge = tuple(edge_index[:, i].tolist())\n",
    "        weight = edge_weight_dict.get(edge, 0)  # Default to 0 if edge not found\n",
    "        weights.append(weight)\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ],
   "id": "92a9f77324216cda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_edge_weights = get_edge_weights(data.train_pos_edge_index, edge_weight_dict)\n",
    "test_edge_weights = get_edge_weights(data.test_pos_edge_index, edge_weight_dict)\n",
    "val_edge_weights = get_edge_weights(data.val_pos_edge_index, edge_weight_dict)"
   ],
   "id": "3482204b0f2c8be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data.train_pos_edge_weight = train_edge_weights\n",
    "data.test_pos_edge_weight = test_edge_weights\n",
    "data.val_pos_edge_weight = val_edge_weights\n",
    "\n",
    "# Manually create negative edges for training\n",
    "neg_edge_index_train = negative_sampling(\n",
    "    edge_index=data.train_pos_edge_index,\n",
    "    num_nodes=data.num_nodes,\n",
    "    num_neg_samples=data.train_pos_edge_index.size(1),\n",
    ")\n",
    "data.train_neg_edge_index = neg_edge_index_train\n",
    "\n",
    "# Assign zero weights to negative edges for training\n",
    "neg_train_edge_weights = torch.zeros(neg_edge_index_train.size(1), dtype=torch.float)\n",
    "\n",
    "# Manually create negative edges for testing\n",
    "neg_edge_index_test = negative_sampling(\n",
    "    edge_index=data.test_pos_edge_index,\n",
    "    num_nodes=data.num_nodes,\n",
    "    num_neg_samples=data.test_pos_edge_index.size(1),\n",
    ")\n",
    "data.test_neg_edge_index = neg_edge_index_test\n",
    "\n",
    "# Assign zero weights to negative edges for testing\n",
    "neg_test_edge_weights = torch.zeros(neg_edge_index_test.size(1), dtype=torch.float)\n",
    "\n",
    "# Combine positive and negative edge weights for training\n",
    "data.train_neg_edge_weight = neg_train_edge_weights\n",
    "\n",
    "# Combine positive and negative edge weights for testing\n",
    "data.test_neg_edge_weight = neg_test_edge_weights\n",
    "\n",
    "# Ensure edge_index tensors are of integer type\n",
    "data.train_pos_edge_index = data.train_pos_edge_index.long()\n",
    "data.test_pos_edge_index = data.test_pos_edge_index.long()\n",
    "data.train_neg_edge_index = data.train_neg_edge_index.long()\n",
    "data.test_neg_edge_index = data.test_neg_edge_index.long()"
   ],
   "id": "91f230b61ba4cc8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define number of nodes\n",
    "num_nodes = data.num_nodes\n",
    "\n",
    "# Node2Vec parameters\n",
    "embedding_dim = 64\n",
    "walk_length = 20\n",
    "context_size = 10\n",
    "walks_per_node = 10\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "num_epochs = 21"
   ],
   "id": "fba21fbb8d038909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Node2Vec\n",
    "node2vec = Node2Vec(\n",
    "    edge_index=original_edge_index,\n",
    "    embedding_dim=embedding_dim,\n",
    "    walk_length=walk_length,\n",
    "    context_size=context_size,\n",
    "    walks_per_node=walks_per_node,\n",
    "    num_negative_samples=1,\n",
    "    p=1,\n",
    "    q=1,\n",
    "    sparse=True,\n",
    "    num_nodes=num_nodes  # Specify number of nodes\n",
    ")\n",
    "\n",
    "# Move to the appropriate device (CPU/GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "node2vec = node2vec.to(device)"
   ],
   "id": "6f325c1adced2d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimizer for Node2Vec\n",
    "optimizer = torch.optim.SparseAdam(node2vec.parameters(), lr=lr)"
   ],
   "id": "d2b1d3e64f0c57b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop for Node2Vec\n",
    "def train_node2vec(num_epochs):\n",
    "    node2vec.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        loader = node2vec.loader(batch_size=batch_size, shuffle=True, num_workers=0)  # Set num_workers=0\n",
    "        for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = node2vec.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i}, Loss: {total_loss / 10}')\n",
    "                total_loss = 0"
   ],
   "id": "a4673e0d68813981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train Node2Vec model\n",
    "train_node2vec(num_epochs)"
   ],
   "id": "36416a190e418fdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract embeddings\n",
    "node_embeddings = node2vec.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "# Update node features with embeddings\n",
    "data.x = torch.tensor(node_embeddings, dtype=torch.float)"
   ],
   "id": "4f2994a9c90aef5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GAE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GAE, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_weight):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        return self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        pos_pred = (z[pos_edge_index[0].long()] * z[pos_edge_index[1].long()]).sum(dim=1)\n",
    "        neg_pred = (z[neg_edge_index[0].long()] * z[neg_edge_index[1].long()]).sum(dim=1)\n",
    "        return pos_pred, neg_pred\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.train_pos_edge_index, data.train_pos_edge_weight)\n",
    "        return z"
   ],
   "id": "280a1a320e603e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize and train GAE model as before\n",
    "model = GAE(data.num_node_features, 16)  # Adjust dimensions as needed\n",
    "gae_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ],
   "id": "d9923f5f9b0fdb86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(data):\n",
    "    model.train()\n",
    "    gae_optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.train_pos_edge_index, data.train_pos_edge_weight)  # Pass train_pos_edge_weight\n",
    "    pos_pred, neg_pred = model.decode(z, data.train_pos_edge_index, data.train_neg_edge_index)\n",
    "    pos_loss = loss_fn(pos_pred, torch.ones_like(pos_pred))\n",
    "    neg_loss = loss_fn(neg_pred, torch.zeros_like(neg_pred))\n",
    "    loss = pos_loss + neg_loss\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(\"Warning: NaN or Inf loss detected\")\n",
    "        return float('inf')\n",
    "    loss.backward()\n",
    "    gae_optimizer.step()\n",
    "    return loss.item()"
   ],
   "id": "d59e18cf369aff0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(100):\n",
    "    loss = train(data)\n",
    "    if loss == float('inf'):\n",
    "        break\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')"
   ],
   "id": "819793948be2b1e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def average_precision(y_true, y_pred):\n",
    "    idx = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[idx]\n",
    "    tp = np.cumsum(y_true_sorted)\n",
    "    precision = tp / (np.arange(len(y_true_sorted)) + 1)\n",
    "    avg_precision = np.sum(precision * y_true_sorted) / np.sum(y_true_sorted)\n",
    "    return avg_precision\n",
    "\n",
    "def mean_average_precision(y_true, y_pred):\n",
    "    return np.mean([average_precision(y_t, y_p) for y_t, y_p in zip(y_true, y_pred)])"
   ],
   "id": "6d8004ed44dd947a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(data, model, k):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.val_pos_edge_index, data.val_pos_edge_weight)\n",
    "        pos_pred = torch.sigmoid((z[data.val_pos_edge_index[0].long()] * z[data.val_pos_edge_index[1].long()]).sum(dim=1)).cpu().numpy()\n",
    "        neg_pred = torch.sigmoid((z[data.val_neg_edge_index[0].long()] * z[data.val_neg_edge_index[1].long()]).sum(dim=1)).cpu().numpy()\n",
    "\n",
    "    y_true = np.concatenate([np.ones(pos_pred.shape[0]), np.zeros(neg_pred.shape[0])])\n",
    "    y_pred = np.concatenate([pos_pred, neg_pred])\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    map_score = mean_average_precision([y_true], [y_pred])\n",
    "\n",
    "    return auc_roc, ap, map_score"
   ],
   "id": "dd65d2d7892ba243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "k=10",
   "id": "a6ac9fd7abb81a9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "auc_roc, ap, map_score = evaluate_model(data, model, k)\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"AP: {ap:.4f}\")\n",
    "print(f\"MAP: {map_score:.4f}\")"
   ],
   "id": "37b2a03df1f5ce83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_best_candidates(job_descriptions, resumes, z, k=1):\n",
    "    job_ids = job_descriptions['job_id'].values\n",
    "    candidate_ids = resumes['candidate_id'].values + len(job_descriptions)\n",
    "\n",
    "    job_indices = job_ids - 1\n",
    "    candidate_indices = candidate_ids - 1\n",
    "\n",
    "    job_embeddings = z[job_indices]\n",
    "    candidate_embeddings = z[candidate_indices]\n",
    "\n",
    "    # Calculate scores using matrix multiplication\n",
    "    scores = torch.sigmoid(torch.matmul(job_embeddings, candidate_embeddings.T)).cpu().numpy()\n",
    "\n",
    "    predictions = []\n",
    "    for i, job_id in enumerate(job_ids):\n",
    "        best_match_indices = scores[i].argsort()[::-1][:k]\n",
    "        for idx in best_match_indices:\n",
    "            candidate_id = resumes.iloc[idx]['candidate_id']\n",
    "            candidate_job_title = resumes.iloc[idx]['job_title']\n",
    "            category = resumes.iloc[idx]['category']\n",
    "            skills = resumes.iloc[idx]['skills']\n",
    "            job_title = job_descriptions.iloc[i]['job_title']\n",
    "            job_skills = job_descriptions.iloc[i]['skills']\n",
    "            score = scores[i][idx]\n",
    "\n",
    "            match_percentage = score * 100  # Assuming the score is between 0 and 1\n",
    "            predictions.append({\n",
    "                \"Job ID\": job_id,\n",
    "                \"Job Title\": le_job_title.inverse_transform([job_title])[0],\n",
    "                \"Candidate ID\": candidate_id,\n",
    "                \"Candidate Job Title\": le_job_title.inverse_transform([candidate_job_title])[0],\n",
    "                \"Candidate Category\": le_category.inverse_transform([category])[0],\n",
    "                \"Match Percentage\": match_percentage,\n",
    "                \"Mutual Skills\": set(job_skills).intersection(set(skills)),\n",
    "                \"Job Skills\": job_skills,\n",
    "                \"Candidate Skills\": skills\n",
    "            })\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    return predictions_df"
   ],
   "id": "a2bdfd90257c0b6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "with torch.no_grad():\n",
    "    z = model.encode(data.x, data.test_pos_edge_index, data.test_pos_edge_weight)"
   ],
   "id": "455e4c57053c84e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_df = predict_best_candidates(job_descriptions, resumes, z)\n",
    "predictions_df = predictions_df[predictions_df['Mutual Skills'].map(len) != 0]\n",
    "predictions_df['Mutual Skills Count'] = predictions_df['Mutual Skills'].map(len)\n",
    "ind = predictions_df['Mutual Skills'].map(len).sort_values(ascending=False).index\n",
    "predictions_df = predictions_df.reindex(ind)"
   ],
   "id": "7a2ba7424bc80603",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions_df.head(10000)",
   "id": "1930b82d20689a44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# torch.save(model.state_dict(), f\"./models/gea-recommendation-system-25k-{auc_roc:.2f}-acc-{uuid.uuid4()}-{time.strftime('%Y%m%d-%H%M%S')}-v4.pth\")",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), f\"./models/gea-recommendation-system-v4-a03-1.pth\")",
   "id": "a5493be53c4edf76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3d6f035b4107326f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
