{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np"
   ],
   "id": "74f7ed965b90fc08",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load datasets\n",
    "job_descriptions = pd.read_csv('./data/processed/job_descriptions_processed.csv')\n",
    "resumes = pd.read_csv('./data/processed/resume-dataset-processed.csv', converters={'skills': literal_eval})\n",
    "\n",
    "# Shuffle job_descriptions and select the first 100 rows\n",
    "job_descriptions = job_descriptions.sample(frac=1, random_state=random_seed).head(100)\n",
    "\n",
    "# Convert 'skills' column to list\n",
    "job_descriptions['skills'] = job_descriptions['skills'].apply(literal_eval)"
   ],
   "id": "4991f8d84ef5ee92",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import warnings\n",
    "import uuid\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "datetime_pattern = \"%Y%m%d-%H%M%S\"\n",
    "\n",
    "nltk.download(['stopwords', 'wordnet'])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load datasets\n",
    "job_descriptions = pd.read_csv('./data/processed/job_descriptions_processed.csv')\n",
    "resumes = pd.read_csv('./data/processed/resume-dataset-processed.csv', converters={'skills': literal_eval})\n",
    "\n",
    "# Shuffle job_descriptions and select the first 100 rows\n",
    "job_descriptions = job_descriptions.sample(frac=1, random_state=random_seed).head(100)\n",
    "\n",
    "# Convert 'skills' column to list\n",
    "job_descriptions['skills'] = job_descriptions['skills'].apply(literal_eval)\n",
    "\n",
    "# One-hot encoding skills\n",
    "all_skills = list(set(sum(job_descriptions['skills'].tolist() + resumes['skills'].tolist(), [])))\n",
    "mlb = MultiLabelBinarizer(classes=all_skills)\n",
    "\n",
    "job_skills_encoded = mlb.fit_transform(job_descriptions['skills'])\n",
    "resume_skills_encoded = mlb.fit_transform(resumes['skills'])\n",
    "\n",
    "job_skills_tensor = torch.tensor(job_skills_encoded, dtype=torch.float)\n",
    "resume_skills_tensor = torch.tensor(resume_skills_encoded, dtype=torch.float)\n",
    "\n",
    "# Function to create feature matrix from selected columns\n",
    "def create_feature_matrix(df, feature_columns):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        features.append(torch.tensor([row[col] for col in feature_columns], dtype=torch.float))\n",
    "    return torch.stack(features)\n",
    "\n",
    "# Create feature matrices for job descriptions and resumes\n",
    "job_exp_features = create_feature_matrix(job_descriptions, ['min_experience'])\n",
    "resume_exp_features = create_feature_matrix(resumes, ['experience'])\n",
    "\n",
    "# Ensure the dimensions of tensors match correctly for concatenation\n",
    "job_features = torch.cat([job_skills_tensor, job_exp_features], dim=1)\n",
    "resume_features = torch.cat([resume_skills_tensor, resume_exp_features], dim=1)\n",
    "\n",
    "x_one_hot = torch.cat([job_features, resume_features], dim=0)\n",
    "\n",
    "# Embedding skills using nn.Embedding\n",
    "skill_to_index = {skill: idx for idx, skill in enumerate(all_skills)}\n",
    "embedding_dim = 50\n",
    "embedding = nn.Embedding(len(all_skills), embedding_dim)\n",
    "\n",
    "# Function to get average embedding for a list of skills\n",
    "def get_skill_embedding(skills, embedding, skill_to_index):\n",
    "    if not skills:  # If the skills list is empty\n",
    "        return torch.zeros(embedding_dim)\n",
    "    skill_indices = [skill_to_index[skill] for skill in skills if skill in skill_to_index]\n",
    "    if not skill_indices:\n",
    "        return torch.zeros(embedding_dim)\n",
    "    skill_tensor = torch.tensor(skill_indices, dtype=torch.long)\n",
    "    skill_embeddings = embedding(skill_tensor)\n",
    "    return skill_embeddings.mean(dim=0)\n",
    "\n",
    "# Encode skills as embeddings\n",
    "job_skills_embedded = torch.stack([get_skill_embedding(skills, embedding, skill_to_index) for skills in job_descriptions['skills']])\n",
    "resume_skills_embedded = torch.stack([get_skill_embedding(skills, embedding, skill_to_index) for skills in resumes['skills']])\n",
    "\n",
    "# Ensure the dimensions of tensors match correctly for concatenation\n",
    "job_features_embedded = torch.cat([job_skills_embedded, job_exp_features], dim=1)\n",
    "resume_features_embedded = torch.cat([resume_skills_embedded, resume_exp_features], dim=1)\n",
    "\n",
    "x_embeddings = torch.cat([job_features_embedded, resume_features_embedded], dim=0)\n",
    "\n",
    "# Function to create edge index for bipartite graph\n",
    "def create_edge_index(job_descriptions, resumes):\n",
    "    edges = []\n",
    "    num_jobs = len(job_descriptions)\n",
    "    num_resumes = len(resumes)\n",
    "\n",
    "    for i, job in job_descriptions.iterrows():\n",
    "        for j, resume in resumes.iterrows():\n",
    "            if set(job['skills']).intersection(set(resume['skills'])):\n",
    "                if i < num_jobs and (j + num_jobs) < (num_jobs + num_resumes):\n",
    "                    edges.append([i, j + num_jobs])  # Offset for bipartite graph\n",
    "\n",
    "    if not edges:  # Ensure there are edges\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Split the indices for job descriptions and resumes\n",
    "num_jobs = len(job_descriptions)\n",
    "num_resumes = len(resumes)\n",
    "\n",
    "# Generate indices for jobs and resumes\n",
    "job_indices = list(range(num_jobs))\n",
    "resume_indices = list(range(num_jobs, num_jobs + num_resumes))\n",
    "\n",
    "# Split indices into train and test sets\n",
    "job_train_indices, job_test_indices = train_test_split(job_indices, test_size=0.2, random_state=42)\n",
    "resume_train_indices, resume_test_indices = train_test_split(resume_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine train and test indices\n",
    "train_indices = job_train_indices + resume_train_indices\n",
    "test_indices = job_test_indices + resume_test_indices\n",
    "\n",
    "# Create mask tensors for training and testing\n",
    "train_mask = torch.zeros(num_jobs + num_resumes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_jobs + num_resumes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "print(f\"train_mask size: {train_mask.size()}\")  # Debug: Print the size of train_mask\n",
    "\n",
    "# Adjust edge_index creation to filter only training data edges\n",
    "def create_edge_index_for_split(job_descriptions, resumes, train_mask):\n",
    "    edges = []\n",
    "    num_jobs = len(job_descriptions)\n",
    "    num_resumes = len(resumes)\n",
    "\n",
    "    for i, job in job_descriptions.iterrows():\n",
    "        for j, resume in resumes.iterrows():\n",
    "            job_idx = i\n",
    "            resume_idx = j + num_jobs\n",
    "            # print(f\"job_idx: {job_idx}, resume_idx: {resume_idx}\")  # Debug: Print the indices\n",
    "            if job_idx < train_mask.size(0) and resume_idx < train_mask.size(0):  # Ensure indices are within bounds\n",
    "                if train_mask[job_idx] and train_mask[resume_idx]:  # Only add edges if both nodes are in the training set\n",
    "                    edges.append([job_idx, resume_idx])  # Offset for bipartite graph\n",
    "\n",
    "    if not edges:  # Ensure there are edges\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create edge index for training data\n",
    "train_edge_index = create_edge_index_for_split(job_descriptions, resumes, train_mask)\n",
    "\n",
    "# Create Data objects for GCN\n",
    "data_train = Data(x=x_embeddings, edge_index=train_edge_index)\n",
    "data_test = Data(x=x_embeddings, edge_index=create_edge_index(job_descriptions, resumes))  # Use full edge_index for evaluation\n",
    "\n",
    "# Targets (assuming binary classification as before)\n",
    "targets = torch.zeros(num_jobs + num_resumes, dtype=torch.long)\n",
    "targets[num_jobs:] = 1\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "input_dim = x_embeddings.size(1)  # Number of input features (50 + 1 = 51)\n",
    "hidden_dim = 16  # Size of hidden layers\n",
    "output_dim = 2  # Size of the output layer\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with detailed debug information\n",
    "def train(data, model, criterion, optimizer, targets, epochs=100, debug=False):\n",
    "    print(\"Train start\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[train_mask], targets[train_mask])\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if debug:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "            print(f'Output at epoch {epoch}: {out[train_mask].detach().numpy()}')\n",
    "    print(\"Train done\")\n",
    "\n",
    "train(data_train, model, criterion, optimizer, targets, epochs=100, debug=True)\n",
    "\n",
    "# Enhanced Evaluation Function\n",
    "def evaluate(model, data, job_descriptions, resumes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        probabilities = F.softmax(out, dim=1)  # Get probabilities\n",
    "        pred = probabilities.argmax(dim=1)\n",
    "\n",
    "        accuracy = accuracy_score(targets[test_mask], pred[test_mask])\n",
    "        precision = precision_score(targets[test_mask], pred[test_mask])\n",
    "        recall = recall_score(targets[test_mask], pred[test_mask])\n",
    "        f1 = f1_score(targets[test_mask], pred[test_mask])\n",
    "\n",
    "        print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "        torch.save(model.state_dict(), f\"./models/gcn-recommendation-system-{accuracy:.2f}-acc-{uuid.uuid4()}-{time.strftime('%Y%m%d-%H%M%S')}-v1.pth\")\n",
    "\n",
    "        # Detailed Matching Results\n",
    "        num_jobs = len(job_descriptions)\n",
    "        num_resumes = len(resumes)\n",
    "        best_matches = {}\n",
    "\n",
    "        for i in range(num_jobs):\n",
    "            print(f\"\\nEvaluating Job {i}: {job_descriptions.iloc[i]['job_title']}\")\n",
    "            best_match_percentage = 0\n",
    "            best_candidate = None\n",
    "            for j in range(num_resumes):\n",
    "                candidate_index = j + num_jobs\n",
    "                match_percentage = probabilities[candidate_index][1].item() * 100  # Match percentage for the positive class\n",
    "                # print(f\"Candidate {j}: Match Percentage {match_percentage:.2f}%\")  # Debug: Print match percentage for each candidate\n",
    "                if match_percentage > best_match_percentage:\n",
    "                    best_match_percentage = match_percentage\n",
    "                    best_candidate = resumes.iloc[j]\n",
    "            if best_candidate is not None:\n",
    "                best_matches[job_descriptions.iloc[i]['job_id']] = (job_descriptions.iloc[i], best_candidate, best_match_percentage)\n",
    "                print(f\"Best candidate for job {i}: {best_candidate['job_title']} with match percentage {best_match_percentage:.2f}%\")\n",
    "\n",
    "        # Creating DataFrame from best_matches\n",
    "        match_strings = []\n",
    "        for job_id, (job, candidate, match_percentage) in best_matches.items():\n",
    "            match_string = f\"Job: {job['job_id']} - {job['job_title']}, Candidate: {candidate['candidate_id']} - {candidate['category']} ({candidate['job_title']}), Match Percentage: {match_percentage:.2f}%, Candidate Skills: {candidate['skills']}\"\n",
    "            match_strings.append(match_string)\n",
    "\n",
    "        matches_df = pd.DataFrame(match_strings, columns=['Best Matches'])\n",
    "\n",
    "        return matches_df\n",
    "\n",
    "best_matches = evaluate(model, data_test, job_descriptions, resumes)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_matches.head()",
   "id": "63a8c2f826127ee9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3d6f035b4107326f",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
