{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download(['stopwords', 'wordnet'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_experience_expectations(experience_str):\n",
    "    match = re.match(r'(\\d+)\\s+to\\s+(\\d+)\\s+Years', experience_str)\n",
    "    if match:\n",
    "        min_exp, pref_exp = match.groups()\n",
    "        return int(min_exp) * 12, int(pref_exp) * 12\n",
    "    return None, None\n",
    "\n",
    "def split_skills(skills_str):\n",
    "    pattern = re.compile(r'([A-Z][a-z]*(?:\\s[a-z]+)*)')\n",
    "    skills = pattern.findall(skills_str)\n",
    "    skills = [skill.lower() for skill in skills]\n",
    "    return skills\n",
    "\n",
    "jobs_df = pd.read_csv('./data/training/job_descriptions.csv')\n",
    "# jobs_df = jobs_df.reindex(np.random.permutation(jobs_df.index))\n",
    "# jobs_df = jobs_df.copy().iloc[0:10,]\n",
    "\n",
    "experience = jobs_df['Experience'].apply(lambda x: pd.Series(parse_experience_expectations(x)))\n",
    "jobs_df = pd.DataFrame({\n",
    "    'job_id': jobs_df['Job Id'],\n",
    "    'job_title': jobs_df['Job Title'].str.lower(),\n",
    "    'skills': jobs_df['skills'].apply(split_skills),\n",
    "    'min_experience': experience[0],\n",
    "    'max_experience': experience[1],\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_spacy_pattern(title):\n",
    "    words = title.split()\n",
    "    pattern = [{\"LOWER\": word.lower()} for word in words]\n",
    "    return {\"label\": \"TITLE\", \"pattern\": pattern}\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "skill_pattern_path = \"./data/preprocessing/jz_skill_patterns.jsonl\"\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "\n",
    "titles_df = pd.read_json('./data/preprocessing/job-titles.json')\n",
    "title_patterns = [create_spacy_pattern(title) for title in titles_df['job-titles'].to_numpy()]\n",
    "ruler.add_patterns(title_patterns)\n",
    "\n",
    "def get_skills(doc):\n",
    "    skills = [ent.text for ent in doc.ents if ent.label_ == \"SKILL\"]\n",
    "    return list(set(skills))\n",
    "\n",
    "def get_title(doc):\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"TITLE\":\n",
    "            return ent.text\n",
    "    return None\n",
    "\n",
    "def extract_experience(cv_text):\n",
    "    # Define a pattern for experience\n",
    "    experience_pattern = r'\\b(\\d+)\\s*(year|month)\\b'\n",
    "\n",
    "    # Find all experience mentions in the CV text\n",
    "    experience_found = re.findall(experience_pattern, cv_text)\n",
    "\n",
    "    # Filter out None or empty entries and sum up experience\n",
    "    total_months = 0\n",
    "    for value, unit in experience_found:\n",
    "        if unit == \"year\":\n",
    "            total_months += int(value) * 12\n",
    "        elif unit == \"month\":\n",
    "            total_months += int(value)\n",
    "\n",
    "    return total_months\n",
    "\n",
    "def clean_text(text):\n",
    "    review = re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', \" \", text)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [lm.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "    return \" \".join(review)\n",
    "\n",
    "resume_df = pd.read_csv('./data/training/resume-dataset.csv')\n",
    "resume_df['candidate_id'] = range(1, len(resume_df) + 1)\n",
    "# resume_df = resume_df.reindex(np.random.permutation(resume_df.index))\n",
    "# resume_df = resume_df.copy().iloc[0:10,]\n",
    "\n",
    "processed_resumes = []\n",
    "for _, row in resume_df.iterrows():\n",
    "    text = row['Resume']\n",
    "    candidate_id = row['candidate_id']\n",
    "    cv_text = clean_text(text)\n",
    "    doc = nlp(cv_text)\n",
    "    skills = get_skills(doc)\n",
    "    title = get_title(doc)\n",
    "    experience_data = extract_experience(cv_text)\n",
    "    processed_resumes.append({ \"candidate_id\": candidate_id, \"job_title\": title, \"skills\": skills, \"experience\": experience_data })\n",
    "\n",
    "resume_df = pd.DataFrame(processed_resumes)\n",
    "resume_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Example DataFrame structures for job descriptions and resumes\n",
    "# job_descriptions = pd.DataFrame({\n",
    "#     'job_id': [1, 2],\n",
    "#     'skills': [['python', 'ml'], ['java', 'spring']],\n",
    "#     'job_title': ['data scientist', 'backend developer'],\n",
    "#     'min_experience': [12, 24]\n",
    "# })\n",
    "#\n",
    "# resumes = pd.DataFrame({\n",
    "#     'job_title': ['data scientist', 'backend developer'],\n",
    "#     'skills': [['python', 'data analysis'], ['java', 'spring boot']],\n",
    "#     'experience': [24, 36]\n",
    "# })\n",
    "# resumes['candidate_id'] = range(1, len(resumes) + 1)\n",
    "\n",
    "job_descriptions = jobs_df\n",
    "resumes = resume_df\n",
    "job_descriptions.head()\n",
    "resumes.head()\n",
    "\n",
    "# One-hot encoding skills\n",
    "all_skills = list(set(sum(job_descriptions['skills'].tolist() + resumes['skills'].tolist(), [])))\n",
    "mlb = MultiLabelBinarizer(classes=all_skills)\n",
    "job_skills_encoded = mlb.fit_transform(job_descriptions['skills'])\n",
    "resume_skills_encoded = mlb.fit_transform(resumes['skills'])\n",
    "\n",
    "job_skills_tensor = torch.tensor(job_skills_encoded, dtype=torch.float)\n",
    "resume_skills_tensor = torch.tensor(resume_skills_encoded, dtype=torch.float)\n",
    "\n",
    "# Function to create feature matrix from selected columns\n",
    "def create_feature_matrix(df, feature_columns):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        features.append(torch.tensor([row[col] for col in feature_columns], dtype=torch.float))\n",
    "    return torch.stack(features)\n",
    "\n",
    "# Create feature matrices for job descriptions and resumes\n",
    "job_exp_features = create_feature_matrix(job_descriptions, ['min_experience'])\n",
    "resume_exp_features = create_feature_matrix(resumes, ['experience'])\n",
    "\n",
    "# Ensure the dimensions of tensors match correctly for concatenation\n",
    "job_features = torch.cat([job_skills_tensor, job_exp_features], dim=1)\n",
    "resume_features = torch.cat([resume_skills_tensor, resume_exp_features], dim=1)\n",
    "\n",
    "x_one_hot = torch.cat([job_features, resume_features], dim=0)\n",
    "\n",
    "# Embedding skills using nn.Embedding\n",
    "skill_to_index = {skill: idx for idx, skill in enumerate(all_skills)}\n",
    "embedding_dim = 50\n",
    "embedding = nn.Embedding(len(all_skills), embedding_dim)\n",
    "\n",
    "# Function to get average embedding for a list of skills\n",
    "def get_skill_embedding(skills, embedding, skill_to_index):\n",
    "    if not skills:  # If the skills list is empty\n",
    "        return torch.zeros(embedding_dim)\n",
    "    skill_indices = [skill_to_index[skill] for skill in skills if skill in skill_to_index]\n",
    "    if not skill_indices:\n",
    "        return torch.zeros(embedding_dim)\n",
    "    skill_tensor = torch.tensor(skill_indices, dtype=torch.long)\n",
    "    skill_embeddings = embedding(skill_tensor)\n",
    "    return skill_embeddings.mean(dim=0)\n",
    "\n",
    "# Encode skills as embeddings\n",
    "job_skills_embedded = torch.stack([get_skill_embedding(skills, embedding, skill_to_index) for skills in job_descriptions['skills']])\n",
    "resume_skills_embedded = torch.stack([get_skill_embedding(skills, embedding, skill_to_index) for skills in resumes['skills']])\n",
    "\n",
    "# Ensure the dimensions of tensors match correctly for concatenation\n",
    "job_features_embedded = torch.cat([job_skills_embedded, job_exp_features], dim=1)\n",
    "resume_features_embedded = torch.cat([resume_skills_embedded, resume_exp_features], dim=1)\n",
    "\n",
    "x_embeddings = torch.cat([job_features_embedded, resume_features_embedded], dim=0)\n",
    "\n",
    "# Function to create edge index for bipartite graph\n",
    "def create_edge_index(job_descriptions, resumes):\n",
    "    edges = []\n",
    "    num_jobs = len(job_descriptions)\n",
    "    num_resumes = len(resumes)\n",
    "\n",
    "    for i, job in job_descriptions.iterrows():\n",
    "        for j, resume in resumes.iterrows():\n",
    "            if set(job['skills']).intersection(set(resume['skills'])):\n",
    "                if i < num_jobs and (j + num_jobs) < (num_jobs + num_resumes):\n",
    "                    edges.append([i, j + num_jobs])  # Offset for bipartite graph\n",
    "\n",
    "    if not edges:  # Ensure there are edges\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "edge_index = create_edge_index(job_descriptions, resumes)\n",
    "\n",
    "# Create Data objects for GCN\n",
    "data_embeddings = Data(x=x_embeddings, edge_index=edge_index)\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "input_dim = x_embeddings.size(1)  # Number of input features (50 + 1 = 51)\n",
    "hidden_dim = 16  # Size of hidden layers\n",
    "output_dim = 2  # Size of the output layer\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create realistic targets for binary classification\n",
    "num_jobs = len(job_descriptions)\n",
    "num_resumes = len(resumes)\n",
    "targets = torch.zeros(num_jobs + num_resumes, dtype=torch.long)\n",
    "targets[num_jobs:] = 1\n",
    "\n",
    "# Training loop with detailed debug information\n",
    "def train(data, model, criterion, optimizer, targets, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        print(f'Output at epoch {epoch}: {out}')  # Debug: Print the model output\n",
    "        loss = criterion(out, targets)\n",
    "        print(f'Loss at epoch {epoch}: {loss}')  # Debug: Print the loss\n",
    "        loss.backward(retain_graph=True)  # Backward pass\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "train(data_embeddings, model, criterion, optimizer, targets)\n",
    "\n",
    "# Enhanced Evaluation Function\n",
    "def evaluate(model, data, job_descriptions, resumes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        probabilities = F.softmax(out, dim=1)  # Get probabilities\n",
    "        pred = probabilities.argmax(dim=1)\n",
    "\n",
    "        accuracy = accuracy_score(targets, pred)\n",
    "        precision = precision_score(targets, pred)\n",
    "        recall = recall_score(targets, pred)\n",
    "        f1 = f1_score(targets, pred)\n",
    "\n",
    "        print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "        # Detailed Matching Results\n",
    "        num_jobs = len(job_descriptions)\n",
    "        num_resumes = len(resumes)\n",
    "        matches = []\n",
    "        for i in range(num_jobs):\n",
    "            for j in range(num_resumes):\n",
    "                candidate_index = j + num_jobs\n",
    "                match_percentage = probabilities[candidate_index][1].item() * 100  # Match percentage for the positive class\n",
    "                if pred[candidate_index] == 1:  # If candidate is predicted to match a job\n",
    "                    matches.append((job_descriptions.iloc[i], resumes.iloc[j], match_percentage))\n",
    "\n",
    "        print(\"Matching Results:\")\n",
    "        for job, candidate, match_percentage in matches:\n",
    "            print(f\"Job: {job['job_id']} - {job['job_title']}, Candidate: {candidate['candidate_id']} - {candidate['job_title']}, Match Percentage: {match_percentage:.2f}%\")\n",
    "\n",
    "evaluate(model, data_embeddings, job_descriptions, resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
